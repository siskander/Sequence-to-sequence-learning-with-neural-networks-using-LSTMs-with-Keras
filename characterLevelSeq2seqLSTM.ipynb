{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: import the necessary modules\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2, training parameters\n",
    "\n",
    "batch_size = 64 # batch size (number of training samples/epoch) for training\n",
    "epochs = 100 # number of epochs to train for\n",
    "latent_dim = 256 # latent dimensionality of the encoding space\n",
    "num_samples = 10000 #number of samples (sentences) to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/3734379/Desktop/fra.txt\n"
     ]
    }
   ],
   "source": [
    "#Step 3, File dialog  \n",
    "\n",
    "#conda install -c anaconda tk \n",
    "#https://pythonspot.com/tk-file-dialogs/\n",
    "#Python3: tkinter, Python2: Tkinter\n",
    "\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    " \n",
    "root = Tk()\n",
    "root.filename =  filedialog.askopenfilename(initialdir = \"/\",\n",
    "                                            title = \"Select file\",\n",
    "                                            filetypes = ((\"Text File\",\"*.txt\"),(\"All Files\",\"*.*\")))\n",
    "print (root.filename)\n",
    "\n",
    "# path to the data txt file on disk\n",
    "data_path = root.filename\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples (sentences): 10000\n",
      "Number of unique input tokens (# unique characters for input): 69\n",
      "Number of unique output tokens (# unique characters for output): 93\n",
      "Max sequence length for inputs (max sentence length for input): 16\n",
      "Max sequence length for outputs (max sentence length for output): 59\n"
     ]
    }
   ],
   "source": [
    "#Step 4, vectorize the data\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "#open the file as read text\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    #extract all the lines in a list from the dataset file\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "#for each line \n",
    "for line in lines[: min(num_samples, len(lines)-1)]: # iterate over first 1000 lines or less\n",
    "    #from each line extract 2 sentences separated by tab \n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    #tab and new line will be start and end sequences, respectively.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    #append the current input and target sentence to the input and target lists, respectively\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    #construct the character set for input and target\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "#convert the input character set into a sorted character list, and the same for target set\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "#number of character set elements for input (num enoder tokens), and target (num decoder tokens)\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "#maximum sentence length for input and target\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples (sentences):', len(input_texts))\n",
    "print('Number of unique input tokens (# unique characters for input):', num_encoder_tokens)\n",
    "print('Number of unique output tokens (# unique characters for output):', num_decoder_tokens)\n",
    "print('Max sequence length for inputs (max sentence length for input):', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs (max sentence length for output):', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, '$': 2, '%': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '3': 11, '5': 12, '6': 13, '7': 14, '8': 15, '9': 16, ':': 17, '?': 18, 'A': 19, 'B': 20, 'C': 21, 'D': 22, 'E': 23, 'F': 24, 'G': 25, 'H': 26, 'I': 27, 'J': 28, 'K': 29, 'L': 30, 'M': 31, 'N': 32, 'O': 33, 'P': 34, 'Q': 35, 'R': 36, 'S': 37, 'T': 38, 'U': 39, 'V': 40, 'W': 41, 'Y': 42, 'a': 43, 'b': 44, 'c': 45, 'd': 46, 'e': 47, 'f': 48, 'g': 49, 'h': 50, 'i': 51, 'j': 52, 'k': 53, 'l': 54, 'm': 55, 'n': 56, 'o': 57, 'p': 58, 'q': 59, 'r': 60, 's': 61, 't': 62, 'u': 63, 'v': 64, 'w': 65, 'x': 66, 'y': 67, 'z': 68}\n",
      "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, ',': 10, '-': 11, '.': 12, '0': 13, '1': 14, '3': 15, '5': 16, '8': 17, '9': 18, ':': 19, '?': 20, 'A': 21, 'B': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'J': 30, 'K': 31, 'L': 32, 'M': 33, 'N': 34, 'O': 35, 'P': 36, 'Q': 37, 'R': 38, 'S': 39, 'T': 40, 'U': 41, 'V': 42, 'Y': 43, 'a': 44, 'b': 45, 'c': 46, 'd': 47, 'e': 48, 'f': 49, 'g': 50, 'h': 51, 'i': 52, 'j': 53, 'k': 54, 'l': 55, 'm': 56, 'n': 57, 'o': 58, 'p': 59, 'q': 60, 'r': 61, 's': 62, 't': 63, 'u': 64, 'v': 65, 'w': 66, 'x': 67, 'y': 68, 'z': 69, '\\xa0': 70, '«': 71, '»': 72, 'À': 73, 'Ç': 74, 'É': 75, 'Ê': 76, 'à': 77, 'â': 78, 'ç': 79, 'è': 80, 'é': 81, 'ê': 82, 'ë': 83, 'î': 84, 'ï': 85, 'ô': 86, 'ù': 87, 'û': 88, 'œ': 89, '\\u2009': 90, '’': 91, '\\u202f': 92}\n"
     ]
    }
   ],
   "source": [
    "#Step 5, inputCharacter-index lookup table\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "print(input_token_index)\n",
    "\n",
    "#targetCharacters-index lookup table\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "print(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6, turn sentences into 3 numpy arrays\n",
    "#-----------------------------------\n",
    "\n",
    "#size(encoder_input_data) = [rows   : max_input_sentence_length,\n",
    "#                            columns: num_encoder_tokens(characters),\n",
    "#                            depth  : input_text_length(statements number)]\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "#size(decoder_input_data) = [rows   : max_target_sentence_length,\n",
    "#                            columns: num_decoder_tokens(characters),\n",
    "#                            depth  : input_text_length(statements number)]\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "#size(decoder_target_data) = [rows   : max_target_sentence_length,\n",
    "#                             columns: num_decoder_tokens(characters),\n",
    "#                             depth  : input_text_length(statements number)]\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each statement in inputText and targetText\n",
    "#using the zip() operator\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    \n",
    "    #for each char in the input statement\n",
    "    for t, char in enumerate(input_text):\n",
    "        #[statement number, location in the statement, index of char from dict]\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    \n",
    "    #for each char in the output (target) statement\n",
    "    for t, char in enumerate(target_text):\n",
    "        #[statement number, location in the statement, index of char from dict]\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        \n",
    "        #decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        if t>0:\n",
    "            #decoder_target_data will be ahead by one timestep\n",
    "            #and will not include the start character.\n",
    "            ##the current character of decoderInputData is the previous character \n",
    "            ##of decoderTargetData\n",
    "            #[statement number, location in the statement - 1, index of char from dict]\n",
    "            decoder_target_data[i, t-1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.9188 - val_loss: 0.9504\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 77s 10ms/step - loss: 0.7327 - val_loss: 0.7881\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.6214 - val_loss: 0.6981\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.5639 - val_loss: 0.6436\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.5239 - val_loss: 0.6022\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 68s 9ms/step - loss: 0.4921 - val_loss: 0.5800\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 78s 10ms/step - loss: 0.4661 - val_loss: 0.5537\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.4439 - val_loss: 0.5323\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.4242 - val_loss: 0.5151\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.4060 - val_loss: 0.5080\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.3899 - val_loss: 0.4938\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 59s 7ms/step - loss: 0.3753 - val_loss: 0.4839\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.3612 - val_loss: 0.4742\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.3490 - val_loss: 0.4740\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.3371 - val_loss: 0.4619\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.3260 - val_loss: 0.4611\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 0.3148 - val_loss: 0.4601\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 0.3046 - val_loss: 0.4550\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 75s 9ms/step - loss: 0.2951 - val_loss: 0.4583\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 0.2855 - val_loss: 0.4550\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.2769 - val_loss: 0.4573\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.2684 - val_loss: 0.4500\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.2600 - val_loss: 0.4493\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.2519 - val_loss: 0.4573\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.2446 - val_loss: 0.4578\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.2376 - val_loss: 0.4568\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.2306 - val_loss: 0.4653\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.2241 - val_loss: 0.4600\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.2175 - val_loss: 0.4666\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.2115 - val_loss: 0.4727\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.2057 - val_loss: 0.4697\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.2003 - val_loss: 0.4719\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1946 - val_loss: 0.4810\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.1899 - val_loss: 0.4833\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1847 - val_loss: 0.4813\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1797 - val_loss: 0.4917\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.1754 - val_loss: 0.4950\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1712 - val_loss: 0.4998\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1669 - val_loss: 0.5039\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.1629 - val_loss: 0.5174\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1593 - val_loss: 0.5140\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1553 - val_loss: 0.5231\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1519 - val_loss: 0.5263\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1487 - val_loss: 0.5245\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1450 - val_loss: 0.5334\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1423 - val_loss: 0.5399\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1394 - val_loss: 0.5428\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 0.1365 - val_loss: 0.5435\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.1335 - val_loss: 0.5478\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1309 - val_loss: 0.5544\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.1285 - val_loss: 0.5555\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1257 - val_loss: 0.5646\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.1232 - val_loss: 0.5672\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.1214 - val_loss: 0.5715\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1188 - val_loss: 0.5762\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1165 - val_loss: 0.5829\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1145 - val_loss: 0.5831\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.1124 - val_loss: 0.5875\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.1107 - val_loss: 0.5963\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1087 - val_loss: 0.5986\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1068 - val_loss: 0.6028\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.1052 - val_loss: 0.6087\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.1032 - val_loss: 0.6156\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.1018 - val_loss: 0.6147\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1001 - val_loss: 0.6180\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.0984 - val_loss: 0.6176\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.0965 - val_loss: 0.6232\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.0955 - val_loss: 0.6249\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.0938 - val_loss: 0.6334\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.0925 - val_loss: 0.6328\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.0909 - val_loss: 0.6461\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.0899 - val_loss: 0.6481\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.0881 - val_loss: 0.6594\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 59s 7ms/step - loss: 0.0871 - val_loss: 0.6539\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.0859 - val_loss: 0.6519\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 59s 7ms/step - loss: 0.0848 - val_loss: 0.6624\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.0835 - val_loss: 0.6693\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.0822 - val_loss: 0.6658\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.0812 - val_loss: 0.6725\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0798 - val_loss: 0.6749\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.0790 - val_loss: 0.6763\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 76s 10ms/step - loss: 0.0776 - val_loss: 0.6792\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.0769 - val_loss: 0.6865\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.0758 - val_loss: 0.6880\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.0750 - val_loss: 0.6887\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.0735 - val_loss: 0.6902\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.0734 - val_loss: 0.6982\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.0720 - val_loss: 0.6988\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.0712 - val_loss: 0.6986\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.0701 - val_loss: 0.7014\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 68s 9ms/step - loss: 0.0693 - val_loss: 0.7152\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.0682 - val_loss: 0.7133\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.0676 - val_loss: 0.7217\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 76s 10ms/step - loss: 0.0668 - val_loss: 0.7139\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0659 - val_loss: 0.7326\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.0656 - val_loss: 0.7317\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.0643 - val_loss: 0.7248\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 0.0634 - val_loss: 0.7345\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.0629 - val_loss: 0.7353\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 76s 9ms/step - loss: 0.0620 - val_loss: 0.7374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\3734379\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "#Step 7, Construct the LSTM Model\n",
    "#(define the layer type and shape, then apply data to it)\n",
    "# Input layer\n",
    "# LSTM layer\n",
    "# LSTM(output of Input layer)\n",
    "# Dense layer\n",
    "# Dense(output of LSTM layer)\n",
    "\n",
    "#Encode \n",
    "#define an input sequence and process it\n",
    "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "#discard the encoderOutputs and kepp the states\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm= LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "#set up the decoder, using 'encoder_states' as initial state\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#Model\n",
    "#define the model that will turn encoderInputData & decoderInputData into decoderTargetData\n",
    "#the decoder learns to generate targets[t+1 ...] given targets[.. t], conditioned on the input sequence\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data,decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs = epochs,\n",
    "          validation_split=0.2)\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/3734379/Desktop/ABC/Python Codes/s2s.h5\n"
     ]
    }
   ],
   "source": [
    "#File dialog to load the trained model  \n",
    "\n",
    "#conda install -c anaconda tk \n",
    "#https://pythonspot.com/tk-file-dialogs/\n",
    "#Python3: tkinter, Python2: Tkinter\n",
    "\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    " \n",
    "root = Tk()\n",
    "root.filename =  filedialog.askopenfilename(initialdir = \"/\",\n",
    "                                            title = \"Select H5 file\",\n",
    "                                            filetypes = ((\"H5\",\"*.h5\"),(\"All Files\",\"*.*\")))\n",
    "print (root.filename)\n",
    "\n",
    "# path to the data txt file on disk\n",
    "data_path = root.filename\n",
    "root.destroy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference (testing)\n",
    "#--------------------\n",
    "#https://github.com/keras-team/keras/issues/9914\n",
    "#https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq_restore.py\n",
    "\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = load_model(data_path)\n",
    "#----------------------------------------------------------------------------------------\n",
    "#Encoder\n",
    "encoder_inputs = model.input[0]   # load input_1 (encoder input) layer\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output # load lstm_1 (encoder_LSTM) layer\n",
    "encoder_states = [state_h_enc, state_c_enc] #combine the states\n",
    "# Construct Model using encoderInputLayer and encoderStates (from Encoder LSTM layer)\n",
    "# These two layers are already trained. \n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "#---------------------------------------------------------------------------------------\n",
    "#Decoder\n",
    "decoder_inputs = model.input[1]   # load input_2 layer\n",
    "\n",
    "#construct two input layers for the states (acts as a place holder)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "\n",
    "#combine the outputs of the two Input layers\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm = model.layers[3] #load lstm_2\n",
    "\n",
    "#use the trained decoderLTSM with (trained decoder Input layer, 2 states Input layers)\n",
    "#initial_state here is not encoder_states in every time.\n",
    "#This is because after the first prediction, we fed the states of first prediction\n",
    "# to the decoderModel\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h_dec, state_c_dec] #combine the states\n",
    "decoder_dense = model.layers[4] #load the dense layer\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index-inputCharacter lookup table\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "\n",
    "#index_targetCharacter llokup table\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    #Encode the input as state vectors\n",
    "    #let trained Encoder LSTM generates its states, ignore its o/p  \n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    #generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    #populate the first character of target sequence with the start character\n",
    "    #[1, 0, ..., 0]\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        #let trained decoder LSTM generates its o/p and states\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq]+ states_value)\n",
    "        \n",
    "        #Sample the token \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length, or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "  \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input Sentence:  Go.\n",
      "Decoded Sentence:  Va !\n",
      "\n",
      "-\n",
      "Input Sentence:  Hi.\n",
      "Decoded Sentence:  Salut !\n",
      "\n",
      "-\n",
      "Input Sentence:  Run!\n",
      "Decoded Sentence:  Courez !\n",
      "\n",
      "-\n",
      "Input Sentence:  Run!\n",
      "Decoded Sentence:  Courez !\n",
      "\n",
      "-\n",
      "Input Sentence:  Wow!\n",
      "Decoded Sentence:  Ça alors !\n",
      "\n",
      "-\n",
      "Input Sentence:  Fire!\n",
      "Decoded Sentence:  Au feu !\n",
      "\n",
      "-\n",
      "Input Sentence:  Help!\n",
      "Decoded Sentence:  À l'aide !\n",
      "\n",
      "-\n",
      "Input Sentence:  Jump.\n",
      "Decoded Sentence:  Saute.\n",
      "\n",
      "-\n",
      "Input Sentence:  Stop!\n",
      "Decoded Sentence:  Arrête-toi !\n",
      "\n",
      "-\n",
      "Input Sentence:  Stop!\n",
      "Decoded Sentence:  Arrête-toi !\n",
      "\n",
      "-\n",
      "Input Sentence:  Stop!\n",
      "Decoded Sentence:  Arrête-toi !\n",
      "\n",
      "-\n",
      "Input Sentence:  Wait!\n",
      "Decoded Sentence:  Attendez lus !\n",
      "\n",
      "-\n",
      "Input Sentence:  Wait!\n",
      "Decoded Sentence:  Attendez lus !\n",
      "\n",
      "-\n",
      "Input Sentence:  Go on.\n",
      "Decoded Sentence:  Poursuivez.\n",
      "\n",
      "-\n",
      "Input Sentence:  Go on.\n",
      "Decoded Sentence:  Poursuivez.\n",
      "\n",
      "-\n",
      "Input Sentence:  Go on.\n",
      "Decoded Sentence:  Poursuivez.\n",
      "\n",
      "-\n",
      "Input Sentence:  Hello!\n",
      "Decoded Sentence:  Salut !\n",
      "\n",
      "-\n",
      "Input Sentence:  Hello!\n",
      "Decoded Sentence:  Salut !\n",
      "\n",
      "-\n",
      "Input Sentence:  I see.\n",
      "Decoded Sentence:  Je l'ai cre.\n",
      "\n",
      "-\n",
      "Input Sentence:  I try.\n",
      "Decoded Sentence:  J'essaye.\n",
      "\n",
      "-\n",
      "Input Sentence:  I won!\n",
      "Decoded Sentence:  Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input Sentence:  I won!\n",
      "Decoded Sentence:  Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input Sentence:  Oh no!\n",
      "Decoded Sentence:  Oh non !\n",
      "\n",
      "-\n",
      "Input Sentence:  Attack!\n",
      "Decoded Sentence:  Attaque !\n",
      "\n",
      "-\n",
      "Input Sentence:  Attack!\n",
      "Decoded Sentence:  Attaque !\n",
      "\n",
      "-\n",
      "Input Sentence:  Cheers!\n",
      "Decoded Sentence:  Merci !\n",
      "\n",
      "-\n",
      "Input Sentence:  Cheers!\n",
      "Decoded Sentence:  Merci !\n",
      "\n",
      "-\n",
      "Input Sentence:  Cheers!\n",
      "Decoded Sentence:  Merci !\n",
      "\n",
      "-\n",
      "Input Sentence:  Cheers!\n",
      "Decoded Sentence:  Merci !\n",
      "\n",
      "-\n",
      "Input Sentence:  Get up.\n",
      "Decoded Sentence:  Lève-toi.\n",
      "\n",
      "-\n",
      "Input Sentence:  Go now.\n",
      "Decoded Sentence:  Va, maintenant !\n",
      "\n",
      "-\n",
      "Input Sentence:  Go now.\n",
      "Decoded Sentence:  Va, maintenant !\n",
      "\n",
      "-\n",
      "Input Sentence:  Go now.\n",
      "Decoded Sentence:  Va, maintenant !\n",
      "\n",
      "-\n",
      "Input Sentence:  Got it!\n",
      "Decoded Sentence:  Compris !\n",
      "\n",
      "-\n",
      "Input Sentence:  Got it!\n",
      "Decoded Sentence:  Compris !\n",
      "\n",
      "-\n",
      "Input Sentence:  Got it?\n",
      "Decoded Sentence:  Compris ?\n",
      "\n",
      "-\n",
      "Input Sentence:  Got it?\n",
      "Decoded Sentence:  Compris ?\n",
      "\n",
      "-\n",
      "Input Sentence:  Got it?\n",
      "Decoded Sentence:  Compris ?\n",
      "\n",
      "-\n",
      "Input Sentence:  Hop in.\n",
      "Decoded Sentence:  Montez.\n",
      "\n",
      "-\n",
      "Input Sentence:  Hop in.\n",
      "Decoded Sentence:  Montez.\n",
      "\n",
      "-\n",
      "Input Sentence:  Hug me.\n",
      "Decoded Sentence:  Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input Sentence:  Hug me.\n",
      "Decoded Sentence:  Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input Sentence:  I fell.\n",
      "Decoded Sentence:  Je suis tombée.\n",
      "\n",
      "-\n",
      "Input Sentence:  I fell.\n",
      "Decoded Sentence:  Je suis tombée.\n",
      "\n",
      "-\n",
      "Input Sentence:  I know.\n",
      "Decoded Sentence:  Je sais ça.\n",
      "\n",
      "-\n",
      "Input Sentence:  I left.\n",
      "Decoded Sentence:  J'ai plis gnanci.\n",
      "\n",
      "-\n",
      "Input Sentence:  I left.\n",
      "Decoded Sentence:  J'ai plis gnanci.\n",
      "\n",
      "-\n",
      "Input Sentence:  I lost.\n",
      "Decoded Sentence:  J'ai perdu.\n",
      "\n",
      "-\n",
      "Input Sentence:  I'm 19.\n",
      "Decoded Sentence:  J'ai la suelle.\n",
      "\n",
      "-\n",
      "Input Sentence:  I'm OK.\n",
      "Decoded Sentence:  Je suis en train de manger.\n",
      "\n",
      "-\n",
      "Input Sentence:  I'm OK.\n",
      "Decoded Sentence:  Je suis en train de manger.\n",
      "\n",
      "-\n",
      "Input Sentence:  Listen.\n",
      "Decoded Sentence:  Écoutez !\n",
      "\n",
      "-\n",
      "Input Sentence:  No way!\n",
      "Decoded Sentence:  C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence:  No way!\n",
      "Decoded Sentence:  C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence:  No way!\n",
      "Decoded Sentence:  C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence:  No way!\n",
      "Decoded Sentence:  C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence:  No way!\n",
      "Decoded Sentence:  C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence:  No way!\n",
      "Decoded Sentence:  C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence:  No way!\n",
      "Decoded Sentence:  C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence:  No way!\n",
      "Decoded Sentence:  C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence:  No way!\n",
      "Decoded Sentence:  C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence:  Really?\n",
      "Decoded Sentence:  Vrai ?\n",
      "\n",
      "-\n",
      "Input Sentence:  Really?\n",
      "Decoded Sentence:  Vrai ?\n",
      "\n",
      "-\n",
      "Input Sentence:  Really?\n",
      "Decoded Sentence:  Vrai ?\n",
      "\n",
      "-\n",
      "Input Sentence:  Thanks.\n",
      "Decoded Sentence:  Merci !\n",
      "\n",
      "-\n",
      "Input Sentence:  We try.\n",
      "Decoded Sentence:  On essaye.\n",
      "\n",
      "-\n",
      "Input Sentence:  We won.\n",
      "Decoded Sentence:  Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input Sentence:  We won.\n",
      "Decoded Sentence:  Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input Sentence:  We won.\n",
      "Decoded Sentence:  Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input Sentence:  We won.\n",
      "Decoded Sentence:  Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input Sentence:  Ask Tom.\n",
      "Decoded Sentence:  Demande à Tom.\n",
      "\n",
      "-\n",
      "Input Sentence:  Awesome!\n",
      "Decoded Sentence:  Fantastique !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be calm.\n",
      "Decoded Sentence:  Soyez calme !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be calm.\n",
      "Decoded Sentence:  Soyez calme !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be calm.\n",
      "Decoded Sentence:  Soyez calme !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be cool.\n",
      "Decoded Sentence:  Sois détendu !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be fair.\n",
      "Decoded Sentence:  Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be fair.\n",
      "Decoded Sentence:  Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be fair.\n",
      "Decoded Sentence:  Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be fair.\n",
      "Decoded Sentence:  Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be fair.\n",
      "Decoded Sentence:  Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be fair.\n",
      "Decoded Sentence:  Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be kind.\n",
      "Decoded Sentence:  Sois gentil.\n",
      "\n",
      "-\n",
      "Input Sentence:  Be nice.\n",
      "Decoded Sentence:  Soyez gentille !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be nice.\n",
      "Decoded Sentence:  Soyez gentille !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be nice.\n",
      "Decoded Sentence:  Soyez gentille !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be nice.\n",
      "Decoded Sentence:  Soyez gentille !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be nice.\n",
      "Decoded Sentence:  Soyez gentille !\n",
      "\n",
      "-\n",
      "Input Sentence:  Be nice.\n",
      "Decoded Sentence:  Soyez gentille !\n",
      "\n",
      "-\n",
      "Input Sentence:  Beat it.\n",
      "Decoded Sentence:  Dégage !\n",
      "\n",
      "-\n",
      "Input Sentence:  Call me.\n",
      "Decoded Sentence:  Appelle-moi !\n",
      "\n",
      "-\n",
      "Input Sentence:  Call me.\n",
      "Decoded Sentence:  Appelle-moi !\n",
      "\n",
      "-\n",
      "Input Sentence:  Call us.\n",
      "Decoded Sentence:  Appelez-nous !\n",
      "\n",
      "-\n",
      "Input Sentence:  Call us.\n",
      "Decoded Sentence:  Appelez-nous !\n",
      "\n",
      "-\n",
      "Input Sentence:  Come in.\n",
      "Decoded Sentence:  Entrez !\n",
      "\n",
      "-\n",
      "Input Sentence:  Come in.\n",
      "Decoded Sentence:  Entrez !\n",
      "\n",
      "-\n",
      "Input Sentence:  Come in.\n",
      "Decoded Sentence:  Entrez !\n",
      "\n",
      "-\n",
      "Input Sentence:  Come in.\n",
      "Decoded Sentence:  Entrez !\n",
      "\n",
      "-\n",
      "Input Sentence:  Come on!\n",
      "Decoded Sentence:  Allez !\n",
      "\n",
      "-\n",
      "Input Sentence:  Come on.\n",
      "Decoded Sentence:  Venez !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select certain number of statements for testing\n",
    "n = 100\n",
    "\n",
    "for seq_index in range(n):\n",
    "    #take one input sequence (sentence)\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    #input_seq = encoder_input_data[seq_index]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input Sentence: ', input_texts[seq_index])\n",
    "    print('Decoded Sentence: ', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
